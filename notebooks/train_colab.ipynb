# Celda 1: Instalación de Dependencias y Preparación

# 1. Montar Google Drive (Necesario para guardar el modelo .pth)
# Esto te pedirá permiso para acceder a tu Google Drive
from google.colab import drive
drive.mount('/content/drive')

# NOTA: Debes ajustar esta ruta para que apunte a la carpeta 'skiboss' 
# dentro de tu Drive una vez que la hayas subido.
# Por ahora, usamos la ruta local para la instalación de librerías.
PROJECT_PATH = "/content/drive/MyDrive/skiboss" 
import os
# Crear la estructura de directorios en el entorno de Colab
# Esto es esencial para que las importaciones (from backend...) funcionen.
os.makedirs("backend", exist_ok=True)
os.makedirs("saved_models", exist_ok=True)


# 2. Instalar librerías necesarias
# El archivo requirements.txt DEBE estar subido en la raíz del proyecto
!pip install -r /content/drive/MyDrive/skiboss/requirements.txt
# Instalar Stable Baselines 3 (Librería estándar para algoritmos DRL como PPO)
!pip install stable-baselines3 gymnasium


# 3. Copiar el código del backend (SIMULACIÓN)
# Asegúrate de que los archivos se copien desde tu Drive al entorno de Colab
!cp "{PROJECT_PATH}/backend/rl_agent.py" backend/
!cp "{PROJECT_PATH}/backend/indicators.py" backend/
!cp "{PROJECT_PATH}/backend/orderflow.py" backend/
!cp "{PROJECT_PATH}/backend/tradingview_api.py" backend/
!cp "{PROJECT_PATH}/backend/utils.py" backend/  # Si lo has creado, si no, ignorar

print("✅ Entorno preparado. Módulos del backend copiados.")


# Celda 2: Definición del Entorno de Trading (Gymnasium)

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import random
import torch
import torch.nn as nn
from stable_baselines3.common.env_util import make_vec_env

# Importamos las clases de PyTorch y las funciones de análisis del backend
from backend.rl_agent import PolicyNet 
from backend.indicators import calculate_all_features
# from backend.tradingview_api import fetch_historic_data # Usaremos CSV local por la cantidad de datos

# --- Definición del Entorno ---
class TradingEnv(gym.Env):
    metadata = {'render_modes': ['none'], 'render_fps': 30}

    def __init__(self, df: pd.DataFrame, initial_balance=10000):
        super(TradingEnv, self).__init__()
        self.df = df
        self.initial_balance = initial_balance
        self.current_balance = initial_balance
        self.current_step = 0
        self.inventory = 0
        self.history_window = 100 # Necesario para calcular features (ATR, EMA, etc.)
        
        # Espacio de Acción (0=BUY, 1=SELL, 2=HOLD)
        self.action_space = spaces.Discrete(3) 

        # Espacio de Observación (Vector de Features)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(PolicyNet.INPUT_SIZE,), 
            dtype=np.float32
        )

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_balance = self.initial_balance
        # Empezar en un punto aleatorio suficiente para el history_window
        self.current_step = random.randint(self.history_window, len(self.df) - 200) 
        self.inventory = 0
        
        # Obtener el estado inicial
        df_slice = self.df.iloc[:self.current_step]
        observation = calculate_all_features(df_slice) 
        
        return observation, {}

    # Función CRÍTICA: Implementación de la PONDERACIÓN TEMPORAL en la Recompensa
    def _calculate_reward(self, action, is_terminal) -> float:
        """Calcula la recompensa con ponderación a la data reciente."""
        reward = 0
        
        # Simulación de Ganancia/Pérdida (simplificado)
        if self.inventory == 1 and action == 1: # Si tenemos BUY y hacemos SELL
            # Cerrar posición (recompensa = ganancia/pérdida de la operación)
            reward += (self.df['close'].iloc[self.current_step] - self.df['close'].iloc[self.current_step - 1]) * 10
            self.inventory = 0
        elif self.inventory == -1 and action == 0: # Si tenemos SELL y hacemos BUY (cubrir corto)
            reward += (self.df['close'].iloc[self.current_step - 1] - self.df['close'].iloc[self.current_step]) * 10
            self.inventory = 0
        elif action == 0 and self.inventory == 0: # Abrir BUY
            self.inventory = 1
        elif action == 1 and self.inventory == 0: # Abrir SELL
            self.inventory = -1
        
        # PONDERACIÓN TEMPORAL (El requisito crítico)
        # Factor que aumenta exponencialmente con el progreso del set de datos
        total_steps = len(self.df)
        time_factor = (self.current_step / total_steps) 
        
        # Multiplicamos la recompensa por el factor de peso temporal (ej: +50% de peso al final)
        reward *= (1 + 0.5 * time_factor) 

        return reward

    def step(self, action):
        self.current_step += 1
        
        if self.current_step >= len(self.df) - 1:
            terminated = True
            reward = self._calculate_reward(action, True)
            observation = np.zeros(PolicyNet.INPUT_SIZE)
        else:
            terminated = False
            
            # Obtener el nuevo estado (features)
            df_slice = self.df.iloc[:self.current_step + 1]
            observation = calculate_all_features(df_slice) 
            
            # Calcular la recompensa
            reward = self._calculate_reward(action, False)

        return observation, reward, terminated, False, {} 

print("✅ Clase TradingEnv definida (Entorno DRL listo).")


# Celda 3: Lógica de Entrenamiento (Implementación DRL)

from stable_baselines3 import PPO

def reinforcement_learning_training_loop():
    # 1. Carga de datos (20 años simulados)
    print("1. Cargando datos históricos (ej. 20 años de SPY)...")
    
    # --- SIMULACIÓN DE CARGA DE DATOS GRANDES ---
    data_size = 50000 # 50000 pasos (aprox. 20 años de datos diarios)
    df_data = pd.DataFrame({
        'open': np.random.rand(data_size) * 100, 
        'high': np.random.rand(data_size) * 100, 
        'low': np.random.rand(data_size) * 100, 
        'close': np.random.rand(data_size) * 100,
        'volume': np.random.rand(data_size) * 100000
    })

    # 2. Inicialización del Entorno Vectorizado
    print(f"2. Inicializando Entorno de Trading con {len(df_data)} barras.")
    # El make_vec_env es útil para acelerar el entrenamiento DRL
    env = make_vec_env(TradingEnv, n_envs=4, env_kwargs={'df': df_data}) 

    # 3. Inicialización del Agente PPO
    print("3. Inicializando Agente PPO...")
    # La política MlpPolicy automáticamente usa la arquitectura definida en PolicyNet.INPUT_SIZE
    model = PPO("MlpPolicy", env, verbose=1, 
                # Se define la arquitectura de la red para que coincida con PolicyNet
                policy_kwargs=dict(net_arch=[dict(pi=[128, 128, 64], vf=[128, 128, 64])])) 
    
    # 4. Entrenamiento
    print("4. Comenzando Entrenamiento (Total 2,000,000 pasos / 12 horas)...")
    # Este tiempo debe ser ajustado para durar cerca de las 12h de Colab
    model.learn(total_timesteps=2000000)

    # 5. Guardado Final del Modelo (SOLO los pesos de PyTorch)
    # Obtenemos los pesos del modelo PyTorch interno (policy.state_dict)
    model_version = "v1" # Aquí deberías iterar el número de versión (v2, v3, etc.)
    save_path = f'/content/drive/MyDrive/skiboss/saved_models/model_{model_version}.pth'
    
    # Guardamos el estado de la red (PolicyNet)
    torch.save(model.policy.state_dict(), save_path)
    
    print(f"✅ Entrenamiento DRL completado. Modelo guardado en: {save_path}")


# Ejecutar el Loop Principal
if __name__ == '__main__':
    # Ejecutar esta función solo si el archivo .pth no existe o si se desea reentrenar
    reinforcement_learning_training_loop()